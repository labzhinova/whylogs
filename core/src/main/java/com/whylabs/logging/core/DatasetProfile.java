package com.whylabs.logging.core;

import com.google.common.base.Preconditions;
import com.google.common.collect.ImmutableList;
import com.google.common.collect.Iterators;
import com.google.common.collect.Lists;
import com.google.common.collect.Ordering;
import com.google.common.collect.Sets;
import com.google.protobuf.ByteString;
import com.whylabs.logging.core.data.ColumnSummary;
import com.whylabs.logging.core.data.DatasetSummary;
import com.whylabs.logging.core.format.ColumnMessage;
import com.whylabs.logging.core.format.ColumnsChunkSegment;
import com.whylabs.logging.core.format.DatasetMetadataSegment;
import com.whylabs.logging.core.format.DatasetProfileMessage;
import com.whylabs.logging.core.format.MessageSegment;
import com.whylabs.logging.core.iterator.ColumnsChunkSegmentIterator;
import java.io.IOException;
import java.io.ObjectInputStream;
import java.io.ObjectOutputStream;
import java.io.Serializable;
import java.time.Instant;
import java.util.ArrayList;
import java.util.Collections;
import java.util.Iterator;
import java.util.List;
import java.util.Map;
import java.util.Objects;
import java.util.UUID;
import java.util.concurrent.ConcurrentHashMap;
import java.util.stream.Collectors;
import lombok.Getter;
import lombok.NonNull;
import lombok.Value;
import lombok.val;

public class DatasetProfile implements Serializable {
  // generated by IntelliJ
  private static final long serialVersionUID = -9221998596693275458L;

  @Getter String name;
  @Getter Instant timestamp;
  // always sorted
  @Getter List<String> tags;
  Map<String, ColumnProfile> columns;

  /**
   * DEVELOPER API. DO NOT USE DIRECTLY
   *
   * @param name dataset name
   * @param timestamp the timestamp
   * @param tags tags of the dataset
   * @param columns the columns that we're copying over. Note that the source of columns should stop
   *     using these column objects as they will back this DatasetProfile instead
   */
  public DatasetProfile(
      @NonNull String name,
      @NonNull Instant timestamp,
      @NonNull List<String> tags,
      @NonNull Map<String, ColumnProfile> columns) {
    this.name = name;
    this.timestamp = timestamp;
    this.columns = new ConcurrentHashMap<>();
    this.tags = ImmutableList.sortedCopyOf(Sets.newHashSet(tags));
    this.columns = new ConcurrentHashMap<>(columns);
  }

  /**
   * Create a new Dataset profile
   *
   * @param name the name of the dataset profile
   * @param timestamp the timestamp. Normally this is associated with the runtime timestamp
   * @param tags the tags to track the dataset with
   */
  public DatasetProfile(
      @NonNull String name, @NonNull Instant timestamp, @NonNull List<String> tags) {
    this(name, timestamp, tags, Collections.emptyMap());
  }

  public DatasetProfile(String name, Instant timestamp) {
    this(name, timestamp, Collections.emptyList());
  }

  public Map<String, ColumnProfile> getColumns() {
    return Collections.unmodifiableMap(columns);
  }

  private void validate() {
    Preconditions.checkNotNull(name);
    Preconditions.checkNotNull(timestamp);
    Preconditions.checkNotNull(columns);
    Preconditions.checkNotNull(tags);
    Preconditions.checkState(
        Ordering.natural().isOrdered(this.tags), "Tags should be sorted %s", this.tags);
  }

  public void track(String columnName, Object data) {
    trackSingleColumn(columnName, data);
  }

  private void trackSingleColumn(String columnName, Object data) {
    val columnProfile = columns.computeIfAbsent(columnName, ColumnProfile::new);
    columnProfile.track(data);
  }

  public <T> void track(Map<String, T> columns) {
    columns.forEach(this::track);
  }

  public DatasetSummary toSummary() {
    validate();

    val intpColumns =
        columns.values().stream()
            .map(Pair::fromColumn)
            .collect(Collectors.toMap(Pair::getName, Pair::getStatistics));

    return DatasetSummary.newBuilder()
        .setName(name)
        .setTimestamp(timestamp.toEpochMilli())
        .putAllColumns(intpColumns)
        .addAllTags(tags)
        .build();
  }

  public Iterator<MessageSegment> toChunkIterator() {
    validate();

    final String marker = name + UUID.randomUUID().toString();

    // first message is the metadata
    val metadataBuilder =
        DatasetMetadataSegment.newBuilder()
            .setName(this.name)
            .setTimestamp(this.timestamp.toEpochMilli())
            .addAllTags(this.tags)
            .setMarker(marker);
    val metadataSegment = MessageSegment.newBuilder().setMetadata(metadataBuilder).build();

    // then we group the columns by size
    val chunkedColumns =
        columns.values().stream()
            .map(ColumnProfile::toProtobuf)
            .map(ColumnMessage.Builder::build)
            .iterator();

    val columnSegmentMessages =
        Iterators.<ColumnsChunkSegment, MessageSegment>transform(
            new ColumnsChunkSegmentIterator(chunkedColumns, marker),
            msg -> MessageSegment.newBuilder().setColumns(msg).build());

    return Iterators.concat(Iterators.singletonIterator(metadataSegment), columnSegmentMessages);
  }

  public DatasetProfile merge(@NonNull DatasetProfile other) {
    this.validate();
    other.validate();

    Preconditions.checkArgument(
        Objects.equals(this.name, other.name),
        "Mismatched name. Current name [%s] is merged with [%s]",
        this.name,
        other.name);
    Preconditions.checkArgument(
        Objects.equals(this.timestamp, other.timestamp),
        "Mismatched timestamp. Current ts [%s] is merged with [%s]",
        this.timestamp,
        other.timestamp);
    Preconditions.checkArgument(
        Objects.equals(this.tags, other.tags),
        "Mismatched tags. Current %s being merged with %s",
        this.tags,
        other.tags);
    val unionColumns = Sets.union(this.columns.keySet(), other.columns.keySet());

    val result = new DatasetProfile(this.name, this.timestamp, this.tags);

    for (String column : unionColumns) {
      val emptyColumn = new ColumnProfile(column);
      val thisColumn = this.columns.getOrDefault(column, emptyColumn);
      val otherColumn = other.columns.getOrDefault(column, emptyColumn);

      result.columns.put(column, thisColumn.merge(otherColumn));
    }

    return result;
  }

  public DatasetProfileMessage.Builder toProtobuf() {
    validate();
    val builder =
        DatasetProfileMessage.newBuilder().setName(name).setTimestamp(timestamp.toEpochMilli());
    columns.forEach((k, v) -> builder.putColumns(k, v.toProtobuf().build()));
    builder.addAllTags(tags);
    return builder;
  }

  public static DatasetProfile fromProtobuf(DatasetProfileMessage message) {
    val tags = Lists.transform(message.getTagsList().asByteStringList(), ByteString::toStringUtf8);
    val timestamp = Instant.ofEpochMilli(message.getTimestamp());
    val ds = new DatasetProfile(message.getName(), timestamp, tags);
    message.getColumnsMap().forEach((k, v) -> ds.columns.put(k, ColumnProfile.fromProtobuf(v)));

    ds.validate();

    return ds;
  }

  private void writeObject(ObjectOutputStream out) throws IOException {
    validate();

    toProtobuf().build().writeDelimitedTo(out);
  }

  private void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {
    val msg = DatasetProfileMessage.parseDelimitedFrom(in);
    this.name = msg.getName();
    this.timestamp = Instant.ofEpochMilli(msg.getTimestamp());
    this.columns = new ConcurrentHashMap<>();
    this.tags = new ArrayList<>();
    msg.getColumnsMap().forEach((k, v) -> this.columns.put(k, ColumnProfile.fromProtobuf(v)));
    for (val tag : msg.getTagsList().asByteStringList()) {
      this.tags.add(tag.toStringUtf8());
    }
    Collections.sort(this.tags);

    this.validate();
  }

  @Value
  static class Pair {

    String name;
    ColumnSummary statistics;

    static Pair fromColumn(ColumnProfile column) {
      return new Pair(column.getColumnName(), column.toColumnSummary());
    }
  }
}
